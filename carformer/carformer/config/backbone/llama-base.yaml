defaults:
  - llama
architectures:
- LlamaForCausalLM
hidden_size: 768
intermediate_size: 2048
max_position_embeddings: 2048
num_attention_heads: 16
num_hidden_layers: 36
vocab_size: 2
init_from_lm_ckp: false
init_name_or_path: 